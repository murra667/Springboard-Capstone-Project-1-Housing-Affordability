{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web-Scraper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from random import randint\n",
    "from time import time \n",
    "import requests\n",
    "from requests import get\n",
    "from warnings import warn \n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import clear_output\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "import csv \n",
    "\n",
    "# set errors to track number of erros\n",
    "errors = 0\n",
    "\n",
    "# track time during web scraping\n",
    "start_time = time()\n",
    "\n",
    "# track number of requests\n",
    "requests = 0\n",
    "\n",
    "# go through pages of craigslist rentals, divided 120 each page\n",
    "# and seemingly 2400 or less listings in total for the default Minneapolis\n",
    "# craigslist page\n",
    "listings_pages = [str(i) for i in range(0, 2400) if (i%120==0)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# iterate through listing pages which \n",
    "# contain numbers between 0-2400 divisble by 120\n",
    "for page in listings_pages:\n",
    "     \n",
    "    # for page increment value by 120 each time to loop through \n",
    "    # groupings size 120 of craigslist rental ads and make request to url plus\n",
    "    # grouping indicated by number divisible by 120\n",
    "    driver = get('https://minneapolis.craigslist.org/search/apa?s=' + (page))\n",
    "    \n",
    "    \n",
    "    # create beautiful soup object\n",
    "    listing_object = BeautifulSoup(driver.content, 'html.parser')\n",
    "    \n",
    "\n",
    "    # listing containers contains all 120 urls from the url requested with driver variable\n",
    "\n",
    "    listing_containers = listing_object.find_all('li', class_=\"result-row\")\n",
    "\n",
    "    # for each individual url contained in listing_containers\n",
    "    for item in listing_containers:\n",
    "        # increment requests for tracking purposes\n",
    "        requests += 1 \n",
    "        # track time\n",
    "        elapsed_time = time() - start_time\n",
    "        print ('Request: {}; Frequency: {} request/s'.format(requests, requests/elapsed_time))\n",
    "        # clear output for continous display of values\n",
    "        clear_output(wait = True)\n",
    "\n",
    "        # stop loop at 2500 requests\n",
    "        if requests > 2500:\n",
    "            warn('Number of requests was greater than 2280')\n",
    "            break\n",
    "        try:\n",
    "            url = item.find('a', class_=\"result-title hdrlnk\")['href']\n",
    "            \n",
    "        except AttributeError:\n",
    "            print ('cant append')\n",
    "            \n",
    "            \n",
    "       \n",
    "\n",
    "        \n",
    "        try:\n",
    "            # use requests to make request to url leading to single rental listing\n",
    "            link = get(url)\n",
    "            \n",
    "            # create Beatufiul Soup object\n",
    "            listing_object = BeautifulSoup(link.content, 'html.parser')\n",
    "            \n",
    "            # find everything in the \"section tag of each url\n",
    "            page_items = listing_object.find_all('section', class_=\"page-container\")\n",
    "\n",
    "            # scrape desired attributes by looping through \n",
    "            # \"section\" tag of each url\n",
    "            for container in page_items:\n",
    "                # these lists will become the columns for the data frame \n",
    "                # columns that they become are commented above each respective list\n",
    "                # the url list is not here but the variable is already set during each iteration as the \n",
    "                \n",
    "                # 'price'\n",
    "                prices = []\n",
    "                # 'beds_baths'\n",
    "                bed_baths = []\n",
    "                # 'address'\n",
    "                addrs = []\n",
    "                # 'latitude'\n",
    "                lats = []\n",
    "                # 'longitude'\n",
    "                lons = []\n",
    "                # 'map accuracy'\n",
    "                accuracy = []\n",
    "                # 'datetime'\n",
    "                date_times = []\n",
    "                # 'title'\n",
    "                data_titles = []\n",
    "                # 'post id'\n",
    "                posting_ids = []\n",
    "                # 'square feet'\n",
    "                areas = []\n",
    "                # 'square feet 2'\n",
    "                title_price_and_attributes = []\n",
    "                \n",
    "\n",
    "               \n",
    "                # get price\n",
    "                try:\n",
    "                    price = container.h2.find(\"span\", class_=\"price\").text\n",
    "                    prices.append(price)\n",
    "                except AttributeError:\n",
    "                    prices.append(\"N/A\")\n",
    "                except TypeError:\n",
    "                    prices.append(\"N/A\")\n",
    "\n",
    "                       # get latitutude\n",
    "                try:\n",
    "                    lat = container.find('div', class_=\"viewposting\")['data-latitude']\n",
    "                    lats.append(lat)\n",
    "                except TypeError:\n",
    "                    lats.append(\"N/A\")\n",
    "                except AttributeError:\n",
    "                    lats.append(\"N/A\")\n",
    "\n",
    "                        # get longitude\n",
    "                try:\n",
    "                    lon = container.find('div', class_=\"viewposting\")['data-longitude']\n",
    "                    lons.append(lon)\n",
    "                except TypeError:\n",
    "                    lons.append(\"N/A\")\n",
    "                except AttributeError:\n",
    "                    lons.append(\"N/A\")\n",
    "\n",
    "                        # get map data accuracy\n",
    "                try:\n",
    "                    data_accuracy = container.find('div', class_=\"viewposting\")['data-accuracy']\n",
    "                    accuracy.append(data_accuracy)\n",
    "                except TypeError:\n",
    "                    accuracy.append(\"N/A\")\n",
    "                except AttributeError:\n",
    "                    accuracy.append(\"N/A\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        # get datetime\n",
    "                try:\n",
    "                    date_time = container.find('time', class_=\"date timeago\")['datetime']\n",
    "                    date_times.append(date_time)\n",
    "                except TypeError:\n",
    "                    date_times.append(\"N/A\")\n",
    "                except AttributeError:\n",
    "                    date_times.append(\"N/A\")\n",
    "\n",
    "                    # get bedrooms and baths\n",
    "                try:\n",
    "                    bed_bath = container.section.find_all(\"span\", class_=\"shared-line-bubble\")[0].text\n",
    "                    bed_baths.append(bed_bath)\n",
    "                except AttributeError:\n",
    "                    bed_baths.append(\"N/A\")\n",
    "                except TypeError:\n",
    "                    bed_baths.append(\"N/A\")\n",
    "                except IndexError:\n",
    "                    bed_baths.append(\"N/A\")\n",
    "\n",
    "                # get square feet\n",
    "\n",
    "                try: \n",
    "                    sqrft = container.section.find_all(\"span\", class_=\"shared-line-bubble\")[1].text\n",
    "                    areas.append(sqrft)\n",
    "                except AttributeError:\n",
    "                    areas.append(\"N/A\")\n",
    "                except TypeError:\n",
    "                    areas.append(\"N/A\")\n",
    "                except IndexError:\n",
    "                    areas.append(\"N/A\")\n",
    "\n",
    "\n",
    "\n",
    "                # get address \n",
    "                try:   \n",
    "                    addr = container.section.section.find(\"div\", class_=\"mapaddress\").text\n",
    "                    addrs.append(addr)\n",
    "                except AttributeError:\n",
    "                    addrs.append(\"N/A\")\n",
    "\n",
    "                 # get title\n",
    "                try:\n",
    "                    data_title = container.h2.find(id=\"titletextonly\").text\n",
    "                    data_titles.append(data_title)\n",
    "                except AttributeError:\n",
    "                    data_titles.append(\"N/A\")\n",
    "                    \n",
    "                \n",
    "                    # get posting id\n",
    "                try:\n",
    "                    posting_id = container.find_all('p', class_=\"postinginfo\")[1].text\n",
    "                    posting_ids.append(posting_id)\n",
    "                except AttributeError:\n",
    "                    posting_ids.append(\"N/A\")\n",
    "                except IndexError:\n",
    "                    posting_ids.append(\"N/A\")\n",
    "                \n",
    "                \n",
    "                 # this will be information in square feet 2\n",
    "                try:\n",
    "                    price_and_attributes = container.h2.find('span', class_=\"housing\").text\n",
    "                    title_price_and_attributes.append(price_and_attributes)\n",
    "                except AttributeError:\n",
    "                    title_price_and_attributes.append(\"N/A\")\n",
    "                \n",
    "               \n",
    "\n",
    "                # dataframe is updated in each loop \n",
    "\n",
    "\n",
    "\n",
    "                apartment_data = pd.DataFrame({'Url' : url, \n",
    "                                           'Price': prices,\n",
    "                                           'Address': addrs,\n",
    "                                           'Latitude': lats,\n",
    "                                           'Longitude': lons,\n",
    "                                           'Map Accuracy': accuracy,\n",
    "                                           'Date & Time Stamp': date_times,\n",
    "                                           'Bath/Bed': bed_baths, \n",
    "                                           'Areas' : areas,\n",
    "                                           'Listing Title': data_titles,\n",
    "                                           'ids' : posting_ids,\n",
    "                                           'Header': title_price_and_attributes\n",
    "                                           })\n",
    "                # leave dataframe open and call \"a\" for append option to append values iteratively\n",
    "                with open(\"Craigslist_ApartmentData_Minneapolis_4_17.csv\", \"a\", encoding='utf-8') as f:\n",
    "                    apartment_data.to_csv(f, header=False)\n",
    "        # keep track of requests that come back errors\n",
    "        except:\n",
    "            errors += 1\n",
    "            print(errors)\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE SCRAPED DATA FOR GEOCODING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas to work with dataframes\n",
    "import pandas as pd\n",
    "# import re to use regular expressions\n",
    "import re\n",
    "# import numpy\n",
    "import numpy as np\n",
    "# import glob to bring together multiple csv files\n",
    "import glob\n",
    "\n",
    "# search for csvs containing web-scraped data with the name scheme\n",
    "csvs = glob.glob(r'*Craigslist_ApartmentData_Minneapolis*.csv')\n",
    "\n",
    "# set dfs list to store each dataframe that gets read in\n",
    "dfs = []\n",
    "\n",
    "# iterate through csv files gathered in glob\n",
    "for file in csvs:\n",
    "    # read each csv as a dataframe with the same column names\n",
    "    df1 = pd.read_csv(file, names=['address', 'square feet', 'beds_baths', 'datetime', 'square feet 2', 'latitude', 'title', 'longitude', 'map accuracy', 'price', 'url', 'post id'])\n",
    "    # append dataframe to dfs list\n",
    "    dfs.append(df1)\n",
    "\n",
    "# concatenate dataframes in dfs together vertically\n",
    "df = pd.concat(dfs, axis=0)\n",
    "# check to see if address present, if it is, the boolean will be set to False\n",
    "# this seems counterintuitive\n",
    "df['address_check'] = [bool(re.match('nan', str(cell))) for cell in df['address']]\n",
    "# sort values by address_check column\n",
    "df = df.sort_values(['address_check'])\n",
    "# set an index of integers equal to the length of dataframe rows\n",
    "index_list = list(range(len(df.index)))\n",
    "# make index series a column in dataframe\n",
    "df['index'] = index_list\n",
    "# set index column as index of dataframe\n",
    "df = df.set_index(['index'])\n",
    "\n",
    "# set entire df type as string, as most operations will require data type to be string\n",
    "df = df.astype(str)\n",
    "\n",
    "# drop any post id values that are null\n",
    "postid_no_null = df.dropna(subset = ['post id'])\n",
    "\n",
    "\n",
    "\n",
    "# create dataframe where \"post id\" column is null\n",
    "null_id = df[df['post id'].isnull()]\n",
    "# drop duplicates in null_id dataframe using the \"title\" column\n",
    "null_id = null_id.drop_duplicates(subset=['title'], keep='first')\n",
    "# drop duplicates using \"post id\" column in the dataframe containing no null \"post id\" values\n",
    "postid_no_null = df.drop_duplicates(subset=['post id'], keep='first')\n",
    "# concatenate vertically null_id and postid_no_null dataframes\n",
    "df = pd.concat([null_id, postid_no_null])\n",
    "# drop latitude, longitude, map accuracy columns \n",
    "df = df.drop(columns=['latitude', 'longitude', 'map accuracy'])\n",
    "\n",
    "\n",
    "# extract and store all possible valid square feet or beds values across columns where they might be\n",
    "# using findall function\n",
    "\n",
    "# column to store values of square feet that exist in column\n",
    "df['value_square_feet_two'] = [re.findall('\\d+ft', str(cell)) for cell in df['square feet 2']]\n",
    "# create column of all patterns of square feet that exist in  beds_baths column\n",
    "df['value_square_feet_three'] = [re.findall('\\d+ft', str(cell)) for cell in df['beds_baths']]\n",
    "# get all patterns of beds that exist in \"square feet 2\" column\n",
    "df['number_of_beds_2'] = [re.findall('\\d+br', str(cell)) for cell in df['square feet 2']]\n",
    "# clean values from columns to prepare them to be brought into \"square feet\", \"price\" and \"beds_baths\" columns\n",
    "df['number_of_beds_2'] = df['number_of_beds_2'].astype(str)\n",
    "df['value_square_feet_two'] = df['value_square_feet_two'].astype(str)\n",
    "df['value_square_feet_three'] = df['value_square_feet_three'].astype(str)\n",
    "\n",
    "# create a boolean of whether or not a beds/bath pattern exists in the \"beds_baths\" column\n",
    "df['first_beds_boolean'] = [bool(re.search('\\d+[Bb]', cell)) for cell in df['beds_baths']] \n",
    "# get boolean of whether or not beds value is present in \"square feet 2\" column\n",
    "df['second_beds_boolean'] = [bool(re.search('\\d+br', str(cell))) for cell in df['square feet 2']]\n",
    "# clean value before moving\n",
    "df.loc[df['second_beds_boolean'] == 1, 'number_of_beds_2'] = df['number_of_beds_2'].apply(lambda x: str(x[2:-2]))\n",
    "# where value in \"beds_baths\" is not available, and is available in \"square feet 2\", \n",
    "# fill with the beds value of \"square feet 2\", stored in \"number_of_beds_2\"\n",
    "df.loc[(df['second_beds_boolean'] == 1) & (df['first_beds_boolean'] == 0), 'beds_baths'] = df['number_of_beds_2']\n",
    "# create a boolean of whether or not a square feet pattern exists in the \"square feet\" column\n",
    "df['first_square_feet_boolean'] = [bool(re.search('\\d+ft', str(cell))) for cell in df['square feet']]\n",
    "# boolean to see if row in square feet 2 column contains square feet value\n",
    "df['second_square_feet_boolean'] = [bool(re.search('\\d+ft', str(cell))) for cell in df['square feet 2']]\n",
    "# clean value before moving\n",
    "df.loc[df['second_square_feet_boolean'] == 1, 'value_square_feet_two'] = df['value_square_feet_two'].apply(lambda x: str(x[2:-2]))\n",
    "# where value in \"square feet\" is not available, and is available in \"square feet 2\", \n",
    "# fill with the square feet value of \"square feet 2\", stored in \"value_square_feet_two\"\n",
    "df.loc[(df['first_square_feet_boolean'] == 0) & (df['second_square_feet_boolean'] == 1), 'square feet'] = df['value_square_feet_two']\n",
    "# boolean to see if beds_baths column is True/False for square feet pattern\n",
    "df['third_square_feet_boolean'] = [bool(re.search('\\d+ft', str(cell))) for cell in df['beds_baths']]\n",
    "# clean value before moving\n",
    "df.loc[df['third_square_feet_boolean'] == 1, 'value_square_feet_three'] = df['value_square_feet_three'].apply(lambda x: str(x[2:-2]))\n",
    "# for all values in beds_bath where the square feet pattern is true, \n",
    "# cell value is updated with number in square_feet_one\n",
    "df.loc[(df['first_square_feet_boolean'] == 0) & (df['second_square_feet_boolean'] == 0) & (df['third_square_feet_boolean'] == 1), 'square feet'] = df['value_square_feet_three']\n",
    "# for square feet, beds_baths and price columns, determine if appropriate values are present with boolean\n",
    "# in order to fill these columns with appropriate values in future operations\n",
    "df['square feet boolean'] = [bool(re.search('\\d+ft', str(cell))) for cell in df['square feet']]\n",
    "df['beds boolean'] = [bool(re.search('\\d+[Bb]', str(cell))) for cell in df['beds_baths']]\n",
    "df['price boolean'] = [bool(re.search('\\d+', str(cell))) for cell in df['price']]\n",
    "\n",
    "\n",
    "# get just digits in square feet column \n",
    "df.loc[(df['square feet boolean'] == 1), 'square feet'] = df['square feet'].apply(lambda x: x[0:-3])\n",
    "# get just numerical value of beds_baths\n",
    "df.loc[df['beds_baths'].str.contains(r'B'), 'beds_baths'] = df['beds_baths'].apply(lambda x: x[0])\n",
    "# get just price in \"price\" column\n",
    "df['price'] = df.price.apply(lambda x: x.replace('$', ''))\n",
    "# identify and drop posts associated with the superbowl\n",
    "df['check_superbowl'] = [bool(re.search(\"SUPERBOWL|Superbowl|SUPER BOWL|Super bowl|superbowl|super bowl|Super Bowl\", cell)) for cell in df['title']]\n",
    "df = df.drop(df[df['check_superbowl'] == 1].index)\n",
    "# extract url information from \"url\" column and fill as value in \"county\" column\n",
    "df['county'] = [str(re.findall('org/.../', cell))[6:9] for cell in df['url']]\n",
    "\n",
    "\n",
    "# get latitude and longitude of counties and assign to respective \"county_lat\" and \"county_lon\" coordinates\n",
    "\n",
    "# create county_lat, county_lon columns\n",
    "df['county_state'] = df['county']\n",
    "df['county_state'] = df['county']\n",
    "\n",
    "\n",
    "\n",
    "# create a column county_state with county and state to append to each respective address\n",
    "df['county_state'] = df['county']\n",
    "df.loc[(df['county'] == \"hnp\"), \"county_state\"] = \" ,Hennepin County, Minnesota\"\n",
    "df.loc[(df['county'] == \"ram\"), \"county_state\"] = \" ,Ramsey County, Minnesota\"\n",
    "df.loc[(df['county'] == \"dak\"), \"county_state\"] = \" ,Dakota County, Minnesota\"\n",
    "df.loc[(df['county'] == \"ank\"), \"county_state\"] = \" ,Anoka County, Minnesota\"\n",
    "df.loc[(df['county'] == \"wsh\"), \"county_state\"] = \" ,Washington County, Minnesota\"\n",
    "df.loc[(df['county'] == \"csw\"), \"county_state\"] = \" ,Carver County, Minnesota\"\n",
    "# append the value in county_state to each respective address\n",
    "df.loc[(df['address_check'] == True), 'address'] = df[['address', 'county_state']].apply(''.join, axis=1)\n",
    "# create geocode_lat and geocode_lon columns for geocoding\n",
    "df['geocode_lat'] = \"none\"\n",
    "df['geocode_lon'] = \"none\"\n",
    "\n",
    "# save dataframe without missing addresses to be geocoded\n",
    "# then resume script after values have been geocoded\n",
    "\n",
    "df.to_csv('geocode_craigslist_rentals_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocoding Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from IPython.core.display import clear_output\n",
    "from random import randint\n",
    "from time import time \n",
    "import requests\n",
    "from requests import get\n",
    "from warnings import warn \n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "from itertools import islice\n",
    "csv = \"geocode_craigslist_rentals_2.csv\"\n",
    "\n",
    "  \n",
    "\n",
    "df = pd.read_csv(csv)\n",
    "\n",
    "index_list = list(range(len(df.index)))\n",
    "df['index'] = index_list\n",
    "\n",
    "df = df.set_index(['index'])\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "for index, row in islice(df.iterrows(),5000, 7500):\n",
    "    if (row['geocode_lat'] == \"none\") & (row['geocode_lon'] == \"none\") & (row['address_check'] == False):\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "        try:\n",
    "            key = \"string given by google\"\n",
    "            sleep((2))\n",
    "            gmaps = googlemaps.Client(key= key)\n",
    "            address = str(row['address'])\n",
    "            geocode_result = gmaps.geocode(address)\n",
    "            latitude = geocode_result[0]['geometry']['location']['lat']\n",
    "            longitude = geocode_result[0]['geometry']['location']['lng'] \n",
    "            df.loc[index, \"geocode_lon\"] = str(longitude)\n",
    "            df.loc[index, \"geocode_lat\"] = str(latitude)\n",
    "            if (count %50 ==0):\n",
    "                print (row['geocode_lon'], row['address'])\n",
    "            count += 1\n",
    "            print ('Request: {}'.format(count))\n",
    "            clear_output(wait = True)\n",
    "\n",
    "\n",
    "        except:\n",
    "            if (count %50 ==0):\n",
    "                print (row['geocode_lon'], row['address'])\n",
    "\n",
    "            df.loc[index, \"geocode_lon\"] = \"error\"\n",
    "            df.loc[index, \"geocode_lat\"] = \"error\"\n",
    "\n",
    "\n",
    "            count += 1\n",
    "            print ('Request: {}'.format(count))\n",
    "            clear_output(wait = True)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "    \n",
    "df.to_csv('geocode_craigslist_rentals_2.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Bring Data Together After Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas to work with dataframes\n",
    "import pandas as pd\n",
    "# import re to use regular expressions\n",
    "import re\n",
    "# import numpy\n",
    "import numpy as np\n",
    "# import glob to bring together multiple csv files\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh.plotting as bkp\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "# search for csvs containing web-scraped data with the name scheme\n",
    "csvs = glob.glob(r'*geocode_craigslist*.csv')\n",
    "\n",
    "# set dfs list to store each dataframe that gets read in\n",
    "dfs = []\n",
    "\n",
    "# iterate through csv files gathered in glob\n",
    "for file in csvs:\n",
    "    # read each csv as a dataframe with the same column names\n",
    "    df1 = pd.read_csv(file)\n",
    "    # append dataframe to dfs list\n",
    "    dfs.append(df1)\n",
    "\n",
    "# concatenate dataframes in dfs together vertically\n",
    "df = pd.concat(dfs, axis=0)\n",
    "# set an index of integers equal to the length of dataframe rows\n",
    "index_list = list(range(len(df.index)))\n",
    "# make index series a column in dataframe\n",
    "df['index'] = index_list\n",
    "# set index column as index of dataframe\n",
    "df = df.set_index(['index'])\n",
    "\n",
    "# set entire df type as string, as most operations will require data type to be string\n",
    "df = df.astype(str)\n",
    "\n",
    "\n",
    "# get only rows from ramsey and hennepin county, and only rows where geocode_lat and geocode_lon are not \"none\n",
    "df_clean = df.loc[(df['geocode_lat'] != \"none\") & (df['geocode_lon'] != \"none\")]\n",
    "df_clean = df_clean.loc[(df_clean['county'] == \"ram\") | (df_clean['county'] == \"hnp\")]\n",
    "# remove \"geocode_lat\" and \"geocode_lon\" columns with \"error\" value\n",
    "df_clean = df_clean.loc[(df_clean['geocode_lat'] != \"error\") & (df_clean['geocode_lon'] != \"error\")]\n",
    "# drop columns that will not be necessary for analysis \n",
    "df_clean = df_clean.drop([\"county_state\", \"number_of_beds_1\", \"latitude\", \"longitude\", \"map accuracy\", \"square_feet_three\", \"square_feet_two\", \"square_feet_one\", \"Unnamed: 0\", \"address_check\", \"address_county\", \"address_lat\", \"address_lon\", \"check_superbowl\", \"county_lat\", \"county_lon\", \"square feet 2\", \"value_square_feet_two\", \"value_square_feet_three\", \"number_of_beds_2\", \"first_beds_boolean\", \"second_beds_boolean\", \"first_square_feet_boolean\", \"second_square_feet_boolean\", \"third_square_feet_boolean\"], axis = 1)    \n",
    "df_clean = df_clean.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset dataset to only contain geocoded values in Ramsey and Hennepin County\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "counties_data = \"mn_county_boundaries_1500.shp\"\n",
    "crs = {'init': 'epsg:4326'}\n",
    "counties = gpd.read_file(counties_data)\n",
    "counties = counties.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "df_clean['geocode_lon'] = df_clean['geocode_lon'].astype(float)\n",
    "df_clean['geocode_lat'] = df_clean['geocode_lat'].astype(float)\n",
    "# convert latitude and longitude points into GIS data points\n",
    "geometry = [Point(xy) for xy in zip(df_clean['geocode_lon'], df_clean['geocode_lat'])]\n",
    "\n",
    "locations = gpd.GeoDataFrame(df_clean, crs=crs, geometry=geometry)\n",
    "locations = locations.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "ramsey_hennepin = counties.loc[(counties['CTY_NAME'] == 'Hennepin') | (counties['CTY_NAME'] == 'Ramsey')]\n",
    "\n",
    "counties_subset = gpd.sjoin(locations, ramsey_hennepin, how=\"right\", op='within')\n",
    "counties_subset = counties_subset.iloc[:, :16]\n",
    "# set an index of integers equal to the length of dataframe rows\n",
    "index_list = list(range(len(counties_subset.index)))\n",
    "# make index series a column in dataframe\n",
    "counties_subset['index'] = index_list\n",
    "# set index column as index of dataframe\n",
    "counties_subset = counties_subset.set_index(['index'])\n",
    "counties_subset = pd.DataFrame(counties_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data Cleaning With Geocoded and Subsetted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:256: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:258: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:275: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outliers grouped             outlier\n",
      "beds_baths         \n",
      "1              44.0\n",
      "2             213.0\n",
      "3               9.0\n",
      "4               1.0\n",
      "studio         86.0\n",
      "outlier proportions beds_baths  outlier\n",
      "1           False      9045\n",
      "            True         44\n",
      "2           False      7999\n",
      "            True        213\n",
      "3           False      1724\n",
      "            True          9\n",
      "4           False       513\n",
      "            True          1\n",
      "studio      False      1441\n",
      "            True         86\n",
      "Name: outlier, dtype: int64\n",
      "outlier percents beds_baths  outlier\n",
      "1           False      99.515898\n",
      "            True        0.484102\n",
      "2           False      97.406235\n",
      "            True        2.593765\n",
      "3           False      99.480669\n",
      "            True        0.519331\n",
      "4           False      99.805447\n",
      "            True        0.194553\n",
      "studio      False      94.368042\n",
      "            True        5.631958\n",
      "Name: outlier, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:327: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:328: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "#convert counties_subset beds_baths to string\n",
    "counties_subset['beds_baths'] = counties_subset['beds_baths'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "# create column in clean dataframe to verify if bed value is assigned in code to follow\n",
    "counties_subset['bed assigned'] = False\n",
    "\n",
    "# assign 'bed assigned' column to false for values that need to be inspected\n",
    "counties_subset.loc[(counties_subset['beds_baths'] == \"0\") | (counties_subset['beds_baths'] == \"0.0\") | (counties_subset['beds boolean'] == \"False\"), 'bed assigned'] = False\n",
    "\n",
    "\n",
    "# take a subset of listings without a beds value, with 0 as a bed value, or 0.0\n",
    "# as the titles of this subset will be insepcted to see if they can be assigned a beds value\n",
    "beds_none = counties_subset.loc[(counties_subset['beds_baths'] == \"0\") | (counties_subset['beds_baths'] == \"0.0\") | (counties_subset['beds boolean'] == \"False\")] \n",
    "\n",
    "\n",
    "# drop any listing that has beds, price and square feet values as null\n",
    "beds_none = beds_none.drop(beds_none[(beds_none['beds boolean'] == \"False\") & (beds_none['price boolean'] == \"False\") & (beds_none['square feet boolean'] == \"False\")].index)\n",
    "\n",
    "\n",
    "# create a list of bed values and their index\n",
    "beds_none_list = beds_none.index.values.tolist()\n",
    "# create a list of titles\n",
    "titles_list = beds_none['title'].values.tolist()\n",
    "# zip the two lists so that the two values remain together\n",
    "beds_none_list = list(zip((beds_none_list), titles_list))\n",
    "# match indexes that do not get assigned\n",
    "# and create a unique set later\n",
    "no_match_indices = []\n",
    "# search for titles with \"studio\" in the title\n",
    "pattern = re.compile('studio', re.IGNORECASE)\n",
    "studios = [value if bool(re.search(pattern, str(value[1]))) is True else no_match_indices.append(value[0]) for value in beds_none_list]\n",
    "# search for titles that suggest they are 1 bedroom\n",
    "pattern = re.compile('one bedroom|1 bedroom|1 br|1br', re.IGNORECASE)\n",
    "ones = [value if bool(re.search(pattern, str(value[1]))) is True else no_match_indices.append(value[0]) for value in beds_none_list]\n",
    "\n",
    "# search for titles that suggest they are 2 bedroom\n",
    "pattern = re.compile('two bedroom|2 bedroom|2 br|2br', re.IGNORECASE)\n",
    "twos = [value if bool(re.search(pattern, str(value[1]))) is True else no_match_indices.append(value[0]) for value in beds_none_list]\n",
    "\n",
    "# search for titles that suggest they are 3 bedroom\n",
    "pattern = re.compile('three bedroom|3 bedroom|3 br|3br', re.IGNORECASE)\n",
    "\n",
    "three = [value if bool(re.search(pattern, str(value[1]))) is True else no_match_indices.append(value[0]) for value in beds_none_list]\n",
    "\n",
    "# search for titles that suggest they are 4 bedroom\n",
    "pattern = re.compile('four bedroom|4 bedroom|4 br|4br', re.IGNORECASE)\n",
    "\n",
    "# search for titles that suggest they are 4 bedroom\n",
    "four = [value if bool(re.search(pattern, str(value[1]))) is True else no_match_indices.append(value[0]) for value in beds_none_list]\n",
    "\n",
    "# see if there are any titles that were duplicated across the lists\n",
    "# to check integrity of regular expression pattern matching operations\n",
    "beds_number_assigned = ones + studios + twos + three + four\n",
    "\n",
    "# use a Counter to determine which titles occur more than once in main list of \n",
    "# beds_number_assigned\n",
    "count_beds_number = Counter(beds_number_assigned)\n",
    "count_beds_number = {k:count_beds_number[k] for k in count_beds_number if count_beds_number[k] > 1}\n",
    "\n",
    "# a 12 bedroom listing appears frequently, which doesn't really have a place in the analysis \n",
    "\n",
    "\n",
    "# take a set of no_match indices so that it is unique\n",
    "no_match_indices = set(no_match_indices)\n",
    "# create list of lists of bedroom types that were or were not discovered\n",
    "bed_assignments_list = [studios, ones, twos, three, four]\n",
    "\n",
    "\n",
    "# compare list of indices with no match to dataframe \n",
    "# to indicate where in dataframe a beds none value was not matched\n",
    "no_match = counties_subset.index.isin(no_match_indices)\n",
    "counties_subset['no match'] = no_match\n",
    "# loop through subset lists of beds values\n",
    "increment = 0\n",
    "\n",
    "for item in bed_assignments_list:\n",
    "\n",
    "    # filter nonetypes from list\n",
    "    indices = [x for x in item if x != None]\n",
    "    # get indices of bed values that were assigned using 'titles' column\n",
    "    # so they can be compared with indices of counties_subset\n",
    "\n",
    "    indices = [x for x, y in indices]\n",
    "    \n",
    "    # compare indices assigned using 'titles' column\n",
    "    # to indices assigned \n",
    "    idx = counties_subset.index.isin(indices)\n",
    "\n",
    "\n",
    "    # add 'index match' column to counties_subset dataframe\n",
    "    counties_subset['index match'] = idx\n",
    "\n",
    "\n",
    "    # where 'index match' is true, set 'bed assigned' to yes, if false assign bed to \n",
    "\n",
    "    counties_subset.loc[(counties_subset['index match'] == True), 'bed assigned'] = True\n",
    "\n",
    "    # for studios group, assign value to studio\n",
    "    if increment == 0:\n",
    "        counties_subset.loc[counties_subset['index match'] == True, 'beds_baths'] = \"studio\"\n",
    "      \n",
    "    # for other groups, assign to number of beds\n",
    "    else:\n",
    "    \n",
    "        counties_subset.loc[counties_subset['index match'] == True, 'beds_baths'] = str(increment)\n",
    "    # add 1 to increment\n",
    "    increment += 1\n",
    "    \n",
    "# for all data, make sure that beds_baths column is consistent \n",
    "counties_subset.loc[(counties_subset['beds_baths'] == \"0.0\") | (counties_subset['beds_baths'] == \"0\"), \"beds_baths\"] = \"studio\"\n",
    "counties_subset.loc[(counties_subset['beds_baths'] == \"1.0\"), \"beds_baths\"] = \"1\"\n",
    "counties_subset.loc[(counties_subset['beds_baths'] == \"2.0\"), \"beds_baths\"] = \"2\"\n",
    "counties_subset.loc[(counties_subset['beds_baths'] == \"3.0\"), \"beds_baths\"] = \"3\"\n",
    "counties_subset.loc[(counties_subset['beds_baths'] == \"4.0\"), \"beds_baths\"] = \"4\"\n",
    "    \n",
    "# \"huge 12 bedroom house\" \"massive 12 bedroom house\"\n",
    "\n",
    "\n",
    "\n",
    "# drop all non-valid values from dataframe\n",
    "# in order to run EDA on beds_baths column\n",
    "df = counties_subset\n",
    "# drop bed_bath values that were not matched using the title column from analysis\n",
    "mask = df['no match'] == False\n",
    "df = df[mask]\n",
    "# subset data so as to only include studio - 4 bedrooms\n",
    "df = df.loc[(df['beds_baths'] == \"studio\") | (df['beds_baths'] == \"1\") | (df['beds_baths'] == \"2\") | (df['beds_baths'] == \"3\") | (df['beds_baths'] == \"4\")]\n",
    "\n",
    "\n",
    "\n",
    "# define function that can deal with subset of values of \n",
    "# beds_baths columns and perform various operations such as standard deviation\n",
    "# and mean\n",
    "def beds_statistics(df, column_name, column_boolean, beds_number = False):\n",
    "    # if beds_number parameter is empty, function runs through all beds values\n",
    "    if beds_number is False:\n",
    "        # variable to increment and set as dictionary key\n",
    "        count_beds = 0\n",
    "        # create dictionary to store values \n",
    "        values_dict = {}\n",
    "        # bed values studio -4 exist in dataset\n",
    "        for value in range(5):\n",
    "            # account for \"studio\" value in beds_baths which is a non-int\n",
    "            if value == 0:\n",
    "                count_beds = \"studio\"\n",
    "                subset = df.loc[(df[column_boolean] == \"True\") & (df[\"beds_baths\"] == \"studio\")]\n",
    "                \n",
    "            else:\n",
    "            # subset data by beds value through each run in loop\n",
    "                subset = df.loc[(df[column_boolean] == \"True\") & (df[\"beds_baths\"] == str(count_beds))]\n",
    "            \n",
    "            # make sure subset only includes non-null values of whatever attribute column is input into \n",
    "            # the function\n",
    "            subset = subset.loc[subset[column_boolean] == \"True\"]\n",
    "            \n",
    "            # convert column of input attribute to numeric\n",
    "            subset[column_name] = pd.to_numeric(subset[column_name], errors='coerce')\n",
    "            # get mean of subset column\n",
    "            mean_value = (subset[column_name].mean())\n",
    "            mean_value = round(mean_value, 4)\n",
    "            # get standard deviation of subset column\n",
    "            std_value = np.std(subset[column_name])\n",
    "            std_value = round(std_value, 5)\n",
    "            \n",
    "            #convert to array so as to use iqr function from statsmodel\n",
    "            new_array = subset[column_name].dropna()\n",
    "            new_array = new_array.as_matrix()\n",
    "            # call iqr function\n",
    "            iqr_value = stats.iqr(new_array, axis=None, rng=(25, 75), scale='raw', nan_policy='propagate', interpolation='linear', keepdims=False)\n",
    "            new_array = subset[column_name].dropna()\n",
    "            new_array = new_array.as_matrix()\n",
    "            # get 25th and 75th percentiles of subsetted column \n",
    "            q1, q3 = np.percentile(new_array, [25, 75])\n",
    "            # set lower boundary as 1.5 standard deviations below the 25th percentile as \n",
    "            # than that will produce negative values\n",
    "            lower = q1 - 1.5*(iqr_value)\n",
    "            # set upper boundary as 3 standard deviations above the 75th percentile so as to\n",
    "            # get rid of impossible rent values\n",
    "            upper = q3 + 3.0*(iqr_value)\n",
    "            # range of acceptable values will be those within the lower and upper bounds\n",
    "            outlier_range = (lower, upper)\n",
    "            # add percentiles to output dictionary\n",
    "            percentiles = (q1, q3)\n",
    "            # create dictionary key and nested dictionary as value\n",
    "            values_dict[count_beds] = {'iqr': iqr_value, 'mean': mean_value, 'std': std_value, 'outlier range': outlier_range, \"percentiles\": percentiles}\n",
    "            # account for \"studio\" value in beds_baths column\n",
    "            if count_beds == \"studio\":\n",
    "                count_beds = 0\n",
    "             # increment count of beds values so as to perform operations on next value from beds column\n",
    "            count_beds += 1\n",
    "        return values_dict\n",
    "   \n",
    "    else:\n",
    "        # create dictionary to store values \n",
    "        values_dict = {}\n",
    "        # subset data by bed value entered in as beds_number parameter\n",
    "        mask = df['beds_baths'] == float(beds_number)\n",
    "        subset = df[mask]\n",
    "        # make sure subset only includes non-null values of whatever attribute column is input into \n",
    "        # the function\n",
    "        subset = subset.loc[subset[column_boolean] == \"True\"]\n",
    "        \n",
    "        # convert column of input attribute to numeric\n",
    "        subset[column_name] = pd.to_numeric(subset[column_name], errors='coerce')\n",
    "        # get mean of subset column\n",
    "        mean_value = (subset[column_name].mean())\n",
    "        mean_value = round(mean_value, 2)\n",
    "        # get standard deviation of subset column\n",
    "        std_value = np.std(subset[column_name])\n",
    "        std_value = round(std_value, 5)\n",
    "        subset = subset.dropna()\n",
    "        #conver to array so as to use iqr function from statsmodel\n",
    "        new_array = subset[column_name].dropna()\n",
    "        \n",
    "        # drop na from new array\n",
    "        #new_array.dropna()\n",
    "        # call iqr function\n",
    "        iqr_value = stats.iqr(new_array, axis=None, rng=(25, 75), scale='raw', nan_policy='propagate', interpolation='linear', keepdims=False)\n",
    "        # get 25th and 75th percentiles of subsetted column \n",
    "        q1, q3 = np.percentile(new_array, [25, 75])\n",
    "        # set lower boundary as 1.5 standard deviations below the 25th percentile as \n",
    "        # than that will produce negative values\n",
    "        lower = q1 - 1.5*(iqr_value)\n",
    "        # set upper boundary as 3 standard deviations above the 75th percentile so as to\n",
    "        # get rid of impossible rent values\n",
    "        upper = q3 + 3.0*(iqr_value)\n",
    "        # range of acceptable values will be those within the lower and upper bounds\n",
    "        outlier_range = (lower, upper)\n",
    "        # add percentiles to output dictionary\n",
    "        percentiles = (q1, q3)\n",
    "\n",
    "\n",
    "        # create dictionary key and nested dictionary as value\n",
    "        values_dict[count_beds] = {'iqr': iqr_value, 'mean': mean_value, 'std': std_value, 'outlier range': outlier_range, \"percentiles\": percentiles}\n",
    "     \n",
    "        return values_dict\n",
    "\n",
    "# create dictionaries of descriptive statistics values for price column \n",
    "# subsetted by bed value\n",
    "price_dict = beds_statistics(df, \"price\", \"price boolean\", beds_number = False)\n",
    "\n",
    "# create dictionaries of descriptive statistics values for square feet column \n",
    "# subsetted by bed value\n",
    "sqft_dict = beds_statistics(df, \"square feet\", \"square feet boolean\", beds_number = False)\n",
    "\n",
    "# function to identify outliers in the data using previously defined outlier range\n",
    "def find_outliers(df, column_boolean, column_name, dictionary):\n",
    "    # subset only non-null values of whatever attribute column is input into the \n",
    "    # function and conver to numeric\n",
    "    df = df.loc[df[column_boolean] == \"True\"]\n",
    "    df[column_name] = pd.to_numeric(df[column_name], errors='coerce')\n",
    "    # Set column to identify outliers\n",
    "    df['outlier'] = False\n",
    "    # loop through bed values keys in dictionaries and determine whether values are outliers\n",
    "    for key in dictionary.items():\n",
    "        df.loc[(df[\"beds_baths\"] == str(key[0])) & (df[column_name] < (key[1]['outlier range'][0])), \"outlier\"] = True\n",
    "        df.loc[(df[\"beds_baths\"] == str(key[0])) & (df[column_name] > (key[1]['outlier range'][1])), \"outlier\"] = True\n",
    "    return df\n",
    "        \n",
    "    \n",
    "\n",
    "# create dataframe that identifies outliers using function\n",
    "prices_outliers = find_outliers(df, \"price boolean\", \"price\", price_dict)\n",
    "\n",
    "\n",
    "\n",
    "# set an index of integers equal to the length of dataframe rows\n",
    "index_list = list(range(len(prices_outliers.index)))\n",
    "# make index series a column in dataframe\n",
    "prices_outliers['index'] = index_list\n",
    "# set index column as index of dataframe\n",
    "prices_outliers = prices_outliers.set_index(['index'])\n",
    "\n",
    "\n",
    "# subset dataframe to only contain non-outliers\n",
    "prices_no_outliers = prices_outliers.loc[prices_outliers['outlier'] == False]\n",
    "\n",
    "\n",
    "# create groupby object of outliers to see where they tend to be distributed among beds groupings\n",
    "outliers_grouped = prices_outliers.groupby(['beds_baths']).agg({'outlier' : 'sum' })\n",
    "\n",
    "print (\"outliers grouped\", outliers_grouped)\n",
    "\n",
    "# create groupby objects to see where the proportion of outliers are distributed\n",
    "outliers_props = prices_outliers.groupby('beds_baths')[\"outlier\"].value_counts()\n",
    "outliers_percents = outliers_props.groupby(level=0).apply(lambda x: 100 * x / float(x.sum()))\n",
    "print (\"outlier proportions\", outliers_props)\n",
    "print (\"outlier percents\", outliers_percents)\n",
    "\n",
    "# get the statistics for the price column of dataframe without outliers\n",
    "prices_no_outliers_statistics = beds_statistics(prices_no_outliers, \"price\", \"price boolean\", beds_number = False)\n",
    "# do the same for square feet, but using prices_no_outliers dataframe\n",
    "sqft_no_outliers_statistics = beds_statistics(prices_no_outliers, \"square feet\", \"square feet boolean\", beds_number = False)\n",
    "\n",
    "# using prices_no_outliers_statistics dictionary\n",
    "# create dictionary of standard deviation and mean of \n",
    "# prices and square feet using the per beds grouping\n",
    "# for future use in data analysis with fair market rents\n",
    "price_mean_dict = {}\n",
    "price_std_dict = {}\n",
    "sqft_mean_dict = {}\n",
    "sqft_std_dict = {}\n",
    "for value in prices_no_outliers_statistics.items():\n",
    "    price_mean_dict[str(value[0])] = value[1]['mean']\n",
    "    price_std_dict[str(value[0])] = value[1]['std']\n",
    "for value in sqft_no_outliers_statistics.items():\n",
    "    sqft_mean_dict[value[0]] = value[1]['mean']\n",
    "    sqft_std_dict[value[0]] = value[1]['std']\n",
    "\n",
    "# fill null values with mean from prices_no_outliers_statistics dictionary\n",
    "\n",
    "# declare final dataframe\n",
    "final_df = prices_no_outliers\n",
    "\n",
    "\n",
    "# make sure that price and square feeet values are null where 'price boolean'\n",
    "# indicates False to be able to fill them with mean values\n",
    "final_df.loc[final_df['price boolean'] == \"False\", \"price\"] = np.nan\n",
    "final_df.loc[final_df['square feet boolean'] == \"False\", \"square feet\"] = np.nan\n",
    "\n",
    "# fill null values with the mean values from \n",
    "final_df['price'] = final_df['price'].fillna(df['beds_baths'].apply(lambda x: price_mean_dict.get(x)))\n",
    "final_df['square feet'] = final_df['square feet'].fillna(df['beds_baths'].apply(lambda x: sqft_mean_dict.get(x)))\n",
    "\n",
    "# drop the index_left column to avoid future problems\n",
    "final_df = final_df.drop(columns = ['index_left'])\n",
    "final_df.to_csv('cleaned_data_final.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
